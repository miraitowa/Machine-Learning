{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 'no'], [0, 0, 0, 1, 'no'], [0, 1, 0, 1, 'yes'], [0, 1, 1, 0, 'yes'], [0, 0, 0, 0, 'no'], [1, 0, 0, 0, 'no'], [1, 0, 0, 1, 'no'], [1, 1, 1, 1, 'yes'], [1, 0, 1, 2, 'yes'], [1, 0, 1, 2, 'yes'], [2, 0, 1, 2, 'yes'], [2, 0, 1, 1, 'yes'], [2, 1, 0, 1, 'yes'], [2, 1, 0, 2, 'yes'], [2, 0, 0, 0, 'no']]\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],         #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']\t\t#分类属性\n",
    "    return dataSet, labels                #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    print(dataSet)\n",
    "    print(calcShannonEnt(dataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "最优特征索引值:2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],                        #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']\t\t#分类属性\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    print(\"最优特征索引值:\" + str(chooseBestFeatureToSplit(dataSet)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.311\n",
      "第1个特征的增益为0.123\n",
      "最优特征索引值:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    \"\"\"   \n",
    "    dataSet = [[0, 0, 0, 0, 'no'],                        #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    \"\"\"\n",
    "    dataSet = [[1,1,'yes'],\n",
    "              [1,0,'no'],\n",
    "              [0,1,'no'],\n",
    "              [0,1,'no']]\n",
    "    labels = ['no surfacing','flippers']     \t\t#分类属性\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    print(\"最优特征索引值:\" + str(chooseBestFeatureToSplit(dataSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from math import log\n",
    "import operator\n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'yes'],\n",
    "               [1,1,'yes'],\n",
    "              [1,0,'no'],\n",
    "              [0,1,'no'],\n",
    "              [0,1,'no']]\n",
    "    labels = ['no surfacing','flippers']     \t\t#分类属性\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\"\"\"函数说明：构造子节点\"\"\"\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.key(): classCount[vote] = 0\n",
    "        classCount[vote] +=1\n",
    "    sortedClassCount = sorted(classCount.iteritems(),\\\n",
    "                             key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "       # print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建决策树\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 训练数据集\n",
    "    labels - 分类属性标签\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "Returns:\n",
    "    myTree - 决策树\n",
    "\n",
    "\"\"\"\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]            #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):            #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                                    #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)                #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                            #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                        #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]        #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                #去掉重复的属性值\n",
    "    for value in uniqueVals:                                    #遍历特征，创建决策树。                       \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)\n",
    "    return myTree\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    featLabels = []\n",
    "    myTree = createTree(dataSet, labels, featLabels)\n",
    "    print(myTree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 特征标签\n",
    "\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'yes'],\n",
    "              [1,0,'no'],\n",
    "              [0,1,'no'],\n",
    "              [0,1,'no']]\n",
    "    labels = ['no surfacing','flippers']        #特征标签\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        # print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:统计classList中出现此处最多的元素(类标签)\n",
    "\n",
    "Parameters:\n",
    "    classList - 类标签列表\n",
    "Returns:\n",
    "    sortedClassCount[0][0] - 出现此处最多的元素(类标签)\n",
    "\n",
    "\"\"\"\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:                                        #统计classList中每个元素出现的次数\n",
    "        if vote not in classCount.keys():classCount[vote] = 0   \n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)        #根据字典的值降序排序\n",
    "    return sortedClassCount[0][0]                                #返回classList中出现次数最多的元素\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建决策树\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 训练数据集\n",
    "    labels - 分类属性标签\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "Returns:\n",
    "    myTree - 决策树\n",
    "\n",
    "\"\"\"\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]            #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):            #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                                    #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)                #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                            #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                        #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]        #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                #去掉重复的属性值\n",
    "    for value in uniqueVals:                                    #遍历特征，创建决策树。                       \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)\n",
    "    return myTree\n",
    "\n",
    "\"\"\"\n",
    "函数说明:获取决策树叶子结点的数目\n",
    "\n",
    "Parameters:\n",
    "    myTree - 决策树\n",
    "Returns:\n",
    "    numLeafs - 决策树的叶子结点的数目\n",
    "\n",
    "\"\"\"\n",
    "def getNumLeafs(myTree):\n",
    "    numLeafs = 0                                                #初始化叶子\n",
    "    firstStr = next(iter(myTree))                                #python3中myTree.keys()返回的是dict_keys,不在是list,所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]                                #获取下一组字典\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':                #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:   numLeafs +=1\n",
    "    return numLeafs\n",
    "\n",
    "\"\"\"\n",
    "函数说明:获取决策树的层数\n",
    "\n",
    "Parameters:\n",
    "    myTree - 决策树\n",
    "Returns:\n",
    "    maxDepth - 决策树的层数\n",
    "\n",
    "\"\"\"\n",
    "def getTreeDepth(myTree):\n",
    "    maxDepth = 0                                                #初始化决策树深度\n",
    "    firstStr = next(iter(myTree))                                #python3中myTree.keys()返回的是dict_keys,不在是list,所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]                                #获取下一个字典\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':                #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:   thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth            #更新层数\n",
    "    return maxDepth\n",
    "\n",
    "\"\"\"\n",
    "函数说明:绘制结点\n",
    "\n",
    "Parameters:\n",
    "    nodeTxt - 结点名\n",
    "    centerPt - 文本位置\n",
    "    parentPt - 标注的箭头位置\n",
    "    nodeType - 结点格式\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    arrow_args = dict(arrowstyle=\"<-\")                                            #定义箭头格式\n",
    "    font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)        #设置中文字体\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',    #绘制结点\n",
    "        xytext=centerPt, textcoords='axes fraction',\n",
    "        va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args, FontProperties=font)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:标注有向边属性值\n",
    "\n",
    "Parameters:\n",
    "    cntrPt、parentPt - 用于计算标注位置\n",
    "    txtString - 标注的内容\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]                                            #计算标注位置                   \n",
    "    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:绘制决策树\n",
    "\n",
    "Parameters:\n",
    "    myTree - 决策树(字典)\n",
    "    parentPt - 标注的内容\n",
    "    nodeTxt - 结点名\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "def plotTree(myTree, parentPt, nodeTxt):\n",
    "    decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")                                        #设置结点格式\n",
    "    leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")                                            #设置叶结点格式\n",
    "    numLeafs = getNumLeafs(myTree)                                                          #获取决策树叶结点数目，决定了树的宽度\n",
    "    depth = getTreeDepth(myTree)                                                            #获取决策树层数\n",
    "    firstStr = next(iter(myTree))                                                            #下个字典                                                 \n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)    #中心位置\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)                                                    #标注有向边属性值\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)                                        #绘制结点\n",
    "    secondDict = myTree[firstStr]                                                            #下一个字典，也就是继续绘制子结点\n",
    "    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD                                        #y偏移\n",
    "    for key in secondDict.keys():                               \n",
    "        if type(secondDict[key]).__name__=='dict':                                            #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            plotTree(secondDict[key],cntrPt,str(key))                                        #不是叶结点，递归调用继续绘制\n",
    "        else:                                                                                #如果是叶结点，绘制叶结点，并标注有向边属性值                                             \n",
    "            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建绘制面板\n",
    "\n",
    "Parameters:\n",
    "    inTree - 决策树(字典)\n",
    "Returns:\n",
    "    无\n",
    "\n",
    "\"\"\"\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')                                                    #创建fig\n",
    "    fig.clf()                                                                                #清空fig\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)                                #去掉x、y轴\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))                                            #获取决策树叶结点数目\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))                                            #获取决策树层数\n",
    "    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;                                #x偏移\n",
    "    plotTree(inTree, (0.5,1.0), '')                                                            #绘制决策树\n",
    "    plt.show()                                                                                 #显示绘制结果     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    featLabels = []\n",
    "    myTree = createTree(dataSet, labels, featLabels)\n",
    "    print(myTree)  \n",
    "    createPlot(myTree)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydotplus\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('lenses.txt', 'r') as fr:                                        #加载文件\n",
    "        lenses = [inst.strip().split('\\t') for inst in fr.readlines()]        #处理文件\n",
    "    lenses_target = []                                                        #提取每组数据的类别，保存在列表里\n",
    "    for each in lenses:\n",
    "        lenses_target.append(each[-1])\n",
    "    print(lenses_target)\n",
    "\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']            #特征标签       \n",
    "    lenses_list = []                                                        #保存lenses数据的临时列表\n",
    "    lenses_dict = {}                                                        #保存lenses数据的字典，用于生成pandas\n",
    "    for each_label in lensesLabels:                                            #提取信息，生成字典\n",
    "        for each in lenses:\n",
    "            lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "        lenses_dict[each_label] = lenses_list\n",
    "        lenses_list = []\n",
    "    # print(lenses_dict)                                                        #打印字典信息\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)                                    #生成pandas.DataFrame\n",
    "    # print(lenses_pd)                                                        #打印pandas.DataFrame\n",
    "    le = LabelEncoder()                                                        #创建LabelEncoder()对象，用于序列化           \n",
    "    for col in lenses_pd.columns:                                            #序列化\n",
    "        lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "    # print(lenses_pd)                                                        #打印编码信息\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(max_depth = 4)                        #创建DecisionTreeClassifier()类\n",
    "    clf = clf.fit(lenses_pd.values.tolist(), lenses_target)                    #使用数据，构建决策树\n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(clf, out_file = dot_data,                            #绘制决策树\n",
    "                        feature_names = lenses_pd.keys(),\n",
    "                        class_names = clf.classes_,\n",
    "                        filled=True, rounded=True,\n",
    "                        special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(\"tree.pdf\")                                                #保存绘制好的决策树，以PDF的形式存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['young', 'myope', 'no', 'reduced', 'no lenses'], ['young', 'myope', 'no', 'normal', 'soft'], ['young', 'myope', 'yes', 'reduced', 'no lenses'], ['young', 'myope', 'yes', 'normal', 'hard'], ['young', 'hyper', 'no', 'reduced', 'no lenses'], ['young', 'hyper', 'no', 'normal', 'soft'], ['young', 'hyper', 'yes', 'reduced', 'no lenses'], ['young', 'hyper', 'yes', 'normal', 'hard'], ['pre', 'myope', 'no', 'reduced', 'no lenses'], ['pre', 'myope', 'no', 'normal', 'soft'], ['pre', 'myope', 'yes', 'reduced', 'no lenses'], ['pre', 'myope', 'yes', 'normal', 'hard'], ['pre', 'hyper', 'no', 'reduced', 'no lenses'], ['pre', 'hyper', 'no', 'normal', 'soft'], ['pre', 'hyper', 'yes', 'reduced', 'no lenses'], ['pre', 'hyper', 'yes', 'normal', 'no lenses'], ['presbyopic', 'myope', 'no', 'reduced', 'no lenses'], ['presbyopic', 'myope', 'no', 'normal', 'no lenses'], ['presbyopic', 'myope', 'yes', 'reduced', 'no lenses'], ['presbyopic', 'myope', 'yes', 'normal', 'hard'], ['presbyopic', 'hyper', 'no', 'reduced', 'no lenses'], ['presbyopic', 'hyper', 'no', 'normal', 'soft'], ['presbyopic', 'hyper', 'yes', 'reduced', 'no lenses'], ['presbyopic', 'hyper', 'yes', 'normal', 'no lenses']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'young'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-80c17463ed6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlensesLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prescript'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'astigmatic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tearRate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mlenses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlensesLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\ancaonda\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\ancaonda\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\ancaonda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'young'"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from sklearn import tree\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fr = open('lenses.txt')\n",
    "    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "    print(lenses)\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    lenses = clf.fit(lenses, lensesLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': ['young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic'], 'prescript': ['myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper'], 'astigmatic': ['no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], 'tearRate': ['reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal']}\n",
      "           age prescript astigmatic tearRate\n",
      "0        young     myope         no  reduced\n",
      "1        young     myope         no   normal\n",
      "2        young     myope        yes  reduced\n",
      "3        young     myope        yes   normal\n",
      "4        young     hyper         no  reduced\n",
      "5        young     hyper         no   normal\n",
      "6        young     hyper        yes  reduced\n",
      "7        young     hyper        yes   normal\n",
      "8          pre     myope         no  reduced\n",
      "9          pre     myope         no   normal\n",
      "10         pre     myope        yes  reduced\n",
      "11         pre     myope        yes   normal\n",
      "12         pre     hyper         no  reduced\n",
      "13         pre     hyper         no   normal\n",
      "14         pre     hyper        yes  reduced\n",
      "15         pre     hyper        yes   normal\n",
      "16  presbyopic     myope         no  reduced\n",
      "17  presbyopic     myope         no   normal\n",
      "18  presbyopic     myope        yes  reduced\n",
      "19  presbyopic     myope        yes   normal\n",
      "20  presbyopic     hyper         no  reduced\n",
      "21  presbyopic     hyper         no   normal\n",
      "22  presbyopic     hyper        yes  reduced\n",
      "23  presbyopic     hyper        yes   normal\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('lenses.txt', 'r') as fr:                                        #加载文件\n",
    "        lenses = [inst.strip().split('\\t') for inst in fr.readlines()]        #处理文件\n",
    "    lenses_target = []                                                        #提取每组数据的类别，保存在列表里\n",
    "    for each in lenses:\n",
    "        lenses_target.append(each[-1])\n",
    "\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']            #特征标签       \n",
    "    lenses_list = []                                                        #保存lenses数据的临时列表\n",
    "    lenses_dict = {}                                                        #保存lenses数据的字典，用于生成pandas\n",
    "    for each_label in lensesLabels:                                            #提取信息，生成字典\n",
    "        for each in lenses:\n",
    "            lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "        lenses_dict[each_label] = lenses_list\n",
    "        lenses_list = []\n",
    "    print(lenses_dict)                                                        #打印字典信息\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)                                    #生成pandas.DataFrame\n",
    "    print(lenses_pd)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           age prescript astigmatic tearRate\n",
      "0        young     myope         no  reduced\n",
      "1        young     myope         no   normal\n",
      "2        young     myope        yes  reduced\n",
      "3        young     myope        yes   normal\n",
      "4        young     hyper         no  reduced\n",
      "5        young     hyper         no   normal\n",
      "6        young     hyper        yes  reduced\n",
      "7        young     hyper        yes   normal\n",
      "8          pre     myope         no  reduced\n",
      "9          pre     myope         no   normal\n",
      "10         pre     myope        yes  reduced\n",
      "11         pre     myope        yes   normal\n",
      "12         pre     hyper         no  reduced\n",
      "13         pre     hyper         no   normal\n",
      "14         pre     hyper        yes  reduced\n",
      "15         pre     hyper        yes   normal\n",
      "16  presbyopic     myope         no  reduced\n",
      "17  presbyopic     myope         no   normal\n",
      "18  presbyopic     myope        yes  reduced\n",
      "19  presbyopic     myope        yes   normal\n",
      "20  presbyopic     hyper         no  reduced\n",
      "21  presbyopic     hyper         no   normal\n",
      "22  presbyopic     hyper        yes  reduced\n",
      "23  presbyopic     hyper        yes   normal\n",
      "    age  prescript  astigmatic  tearRate\n",
      "0     2          1           0         1\n",
      "1     2          1           0         0\n",
      "2     2          1           1         1\n",
      "3     2          1           1         0\n",
      "4     2          0           0         1\n",
      "5     2          0           0         0\n",
      "6     2          0           1         1\n",
      "7     2          0           1         0\n",
      "8     0          1           0         1\n",
      "9     0          1           0         0\n",
      "10    0          1           1         1\n",
      "11    0          1           1         0\n",
      "12    0          0           0         1\n",
      "13    0          0           0         0\n",
      "14    0          0           1         1\n",
      "15    0          0           1         0\n",
      "16    1          1           0         1\n",
      "17    1          1           0         0\n",
      "18    1          1           1         1\n",
      "19    1          1           1         0\n",
      "20    1          0           0         1\n",
      "21    1          0           0         0\n",
      "22    1          0           1         1\n",
      "23    1          0           1         0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('lenses.txt', 'r') as fr:                                        #加载文件\n",
    "        lenses = [inst.strip().split('\\t') for inst in fr.readlines()]        #处理文件\n",
    "    lenses_target = []                                                        #提取每组数据的类别，保存在列表里\n",
    "    for each in lenses:\n",
    "        lenses_target.append(each[-1])\n",
    "\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']            #特征标签       \n",
    "    lenses_list = []                                                        #保存lenses数据的临时列表\n",
    "    lenses_dict = {}                                                        #保存lenses数据的字典，用于生成pandas\n",
    "    for each_label in lensesLabels:                                            #提取信息，生成字典\n",
    "        for each in lenses:\n",
    "            lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "        lenses_dict[each_label] = lenses_list\n",
    "        lenses_list = []\n",
    "    # print(lenses_dict)                                                        #打印字典信息\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)                                    #生成pandas.DataFrame\n",
    "    print(lenses_pd)                                                        #打印pandas.DataFrame\n",
    "    le = LabelEncoder()                                                        #创建LabelEncoder()对象，用于序列化            \n",
    "    for col in lenses_pd.columns:                                            #为每一列序列化\n",
    "        lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "    print(lenses_pd)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydotplus\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('lenses.txt', 'r') as fr:                                        #加载文件\n",
    "        lenses = [inst.strip().split('\\t') for inst in fr.readlines()]        #处理文件\n",
    "    lenses_target = []                                                        #提取每组数据的类别，保存在列表里\n",
    "    for each in lenses:\n",
    "        lenses_target.append(each[-1])\n",
    "    print(lenses_target)\n",
    "\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']            #特征标签       \n",
    "    lenses_list = []                                                        #保存lenses数据的临时列表\n",
    "    lenses_dict = {}                                                        #保存lenses数据的字典，用于生成pandas\n",
    "    for each_label in lensesLabels:                                            #提取信息，生成字典\n",
    "        for each in lenses:\n",
    "            lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "        lenses_dict[each_label] = lenses_list\n",
    "        lenses_list = []\n",
    "    # print(lenses_dict)                                                        #打印字典信息\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)                                    #生成pandas.DataFrame\n",
    "    # print(lenses_pd)                                                        #打印pandas.DataFrame\n",
    "    le = LabelEncoder()                                                        #创建LabelEncoder()对象，用于序列化           \n",
    "    for col in lenses_pd.columns:                                            #序列化\n",
    "        lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "    # print(lenses_pd)                                                        #打印编码信息\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(max_depth = 4)                        #创建DecisionTreeClassifier()类\n",
    "    clf = clf.fit(lenses_pd.values.tolist(), lenses_target)                    #使用数据，构建决策树\n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(clf, out_file = dot_data,                            #绘制决策树\n",
    "                        feature_names = lenses_pd.keys(),\n",
    "                        class_names = clf.classes_,\n",
    "                        filled=True, rounded=True,\n",
    "                        special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(\"tree.pdf\")                                                #保存绘制好的决策树，以PDF的形式存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.420\n",
      "第1个特征的增益为0.171\n",
      "最优特征索引值:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from math import log\n",
    "import operator\n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    dataSet - 数据集\n",
    "    labels - 分类属性\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'yes'],\n",
    "               [1,1,'yes'],\n",
    "              [1,0,'no'],\n",
    "              [0,1,'no'],\n",
    "              [0,1,'no']]\n",
    "    labels = ['no surfacing','flippers']     \t\t#分类属性\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    print(\"最优特征索引值:\" + str(chooseBestFeatureToSplit(dataSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
